"""Source code for ccw-manifests-verifier Lambda.

This Lambda verifies that all recently modified CCW manifests in S3 are marked as "COMPLETED" in the
database. If any are not, this Lambda will send an alert to the provided alert SNS Topic (expected
to be the Splunk On Call Topic in prod).
"""

import itertools
import os
from datetime import UTC, datetime, timedelta
from typing import Annotated, Any

import boto3
import psycopg
from aws_lambda_powertools import Logger
from aws_lambda_powertools.utilities import parameters
from aws_lambda_powertools.utilities.typing import LambdaContext
from botocore.config import Config
from psycopg.rows import dict_row
from pydantic import BaseModel, Field, TypeAdapter

REGION = os.environ.get("AWS_CURRENT_REGION", "us-east-1")
BFD_ENVIRONMENT = os.environ.get("BFD_ENVIRONMENT", "")
DB_CLUSTER_NAME = os.environ.get("DB_CLUSTER_NAME", "")
ETL_BUCKET_ID = os.environ.get("ETL_BUCKET_ID", "")
ALERT_TOPIC_ARNS = [arn.strip() for arn in os.environ.get("ALERT_TOPIC_ARNS", "").split(",")]
BOTO_CONFIG = Config(
    region_name=REGION,
    # Instructs boto3 to retry upto 10 times using an exponential backoff
    retries={
        "total_max_attempts": 10,
        "mode": "adaptive",
    },
)

logger = Logger()


class ManifestFilesResultModel(BaseModel):
    """Pydantic model modeling columns returned from the s3_manifest_files database table."""

    s3_key: str
    status: str


class SplunkOnCallNotificationModel(BaseModel):
    """Pydantic model modeling the message that Splunk On Call expects from AWS alerts.

    The Splunk On Call integration we use to send alerts from AWS expects that alerts originate from
    CloudWatch Alarms. This is true for every alert except this one, so rather than installing
    another integration we simply create a fake CloudWatch Alarm message. This is why the serialized
    fields are named strangely and all but the "alert_message" field default to unusual values.
    Additionally, this Lambda is mostly relevant for prod for which alerts will be sent to the
    corresponding Splunk On Call Topic
    """

    alert_message: Annotated[str, Field(serialization_alias="NewStateReason")]
    alert_description: Annotated[
        str,
        Field(
            serialization_alias="AlarmDescription",
            default_factory=lambda: (
                "Pseudo-Alarm alert generated by the ccw-manifests-verifier that alerts when the "
                f"{BFD_ENVIRONMENT} CCW Pipeline fails to load all available manifests over the "
                'previous weekend by Monday at 9 AM ET. See the "alarm reason" for details on '
                "the failing manifests"
            ),
        ),
    ]
    alert_name: Annotated[
        str,
        Field(serialization_alias="AlarmName", default="weekend-data-load-availability-failure"),
    ]
    state: Annotated[str, Field(serialization_alias="NewStateValue", default="ALARM")]
    change_time: Annotated[
        datetime,
        Field(serialization_alias="StateChangeTime", default_factory=lambda: datetime.now(UTC)),
    ]


def create_splunk_sns_notification(
    unprocessed_manifests: list[str],
) -> SplunkOnCallNotificationModel:
    """Create a SplunkOnCallNotificationModel with an appropriate alert message.

    :param unprocessed_manifests: List of unprocessed manifests to alert upon
    :type unprocessed_manifests: list[str]
    :return: A model with a message indicating the unprocessed manifests
    :rtype: SplunkOnCallNotificationModel
    """
    return SplunkOnCallNotificationModel.model_validate({
        "alert_message": (
            f"{BFD_ENVIRONMENT} CCW Pipeline failed to load {len(unprocessed_manifests)}"
            f" manifest(s) over the weekend: {', '.join(unprocessed_manifests)}"
        )
    })


@logger.inject_lambda_context(clear_state=True, log_event=True)
def handler(event: dict[Any, Any], context: LambdaContext) -> None:  # noqa: ARG001
    """Lambda event handler function called when an EventBridge Scheduler Event is received.

    Args:
        event (dict[Any, Any]): EventBridge Scheduler event details. Unused
        context (LambdaContext): Lambda execution context. Unused

    Raises:
        RuntimeError: If any AWS API operations fail
    """
    try:
        if not all([REGION, BFD_ENVIRONMENT, DB_CLUSTER_NAME, ETL_BUCKET_ID]):
            raise RuntimeError("Not all necessary environment variables were defined")

        rds_client = boto3.client("rds", config=BOTO_CONFIG)
        described_clusters = rds_client.describe_db_clusters(
            DBClusterIdentifier=DB_CLUSTER_NAME
        ).get("DBClusters", [])
        try:
            reader_endpoint = next(
                cluster_detail["ReaderEndpoint"]
                for cluster_detail in described_clusters
                if "ReaderEndpoint" in cluster_detail
            )
        except StopIteration as exc:
            raise RuntimeError(
                "describe-db-clusters returned invalid data for "
                f"{DB_CLUSTER_NAME}: {described_clusters!s}"
            ) from exc
        db_username = parameters.get_parameter(
            f"/bfd/{BFD_ENVIRONMENT}/ccw-pipeline/sensitive/db/username", decrypt=True
        )
        db_password = parameters.get_parameter(
            f"/bfd/{BFD_ENVIRONMENT}/ccw-pipeline/sensitive/db/password", decrypt=True
        )

        logger.info(
            "Connecting to %s as Pipeline user in %s cluster...",
            reader_endpoint,
            DB_CLUSTER_NAME,
        )
        with (
            psycopg.connect(
                host=reader_endpoint,
                user=db_username,
                password=db_password,
                port=5432,
                dbname="fhirdb",
            ) as conn,
            conn.cursor(row_factory=dict_row) as curs,
        ):
            logger.info("Connected to %s", reader_endpoint)
            two_weeks_ago = datetime.now(UTC) - timedelta(weeks=2)

            # Retrieve all manifests in the Incoming paths in s3 that were recently modified in the
            # past 2 weeks. These manifests will be reconciled against their status in the database
            logger.info(
                "Discovering manifest(s) last modified after %s from S3 in Incoming...",
                two_weeks_ago.isoformat(),
            )
            s3_resource = boto3.resource("s3", config=BOTO_CONFIG)
            etl_bucket = s3_resource.Bucket(ETL_BUCKET_ID)
            all_incoming_objects = itertools.chain(
                etl_bucket.objects.filter(Prefix="Incoming/"),
                etl_bucket.objects.filter(Prefix="Synthetic/Incoming/"),
            )
            recent_s3_manifest_keys = [
                object.key
                for object in all_incoming_objects
                if "manifest.xml" in object.key
                and object.last_modified.astimezone(UTC) > two_weeks_ago
            ]
            logger.info(
                "Discovered %d S3 manifest(s) modified after %s",
                len(recent_s3_manifest_keys),
                two_weeks_ago.isoformat(),
            )

            # Retrieve all manifests and their state that were discovered in the past 2 weeks from
            # the database. This list will be the source of truth against which the manifests in s3
            # will be verified against
            logger.info(
                "Retrieving manifest(s) discovered since %s from the database",
                two_weeks_ago.isoformat(),
            )
            raw_results = curs.execute(
                """
                SELECT s3_key, status FROM ccw.s3_manifest_files
                WHERE discovery_timestamp > %s;
                """,
                (two_weeks_ago,),
            ).fetchall()
            db_manifest_files = TypeAdapter(list[ManifestFilesResultModel]).validate_python(
                raw_results
            )
            logger.info(
                "Retrieved %d manifest(s) from the database discovered after %s",
                len(db_manifest_files),
                two_weeks_ago.isoformat(),
            )

            # Reconcile the retrieved recently modified s3 manifests in Incoming with the manifests
            # and their state in the database. If any s3 manifests either are not in the COMPLETED
            # state or simply aren't in the database, the Pipeline failed to finish loading over the
            # weekend and we should alert
            logger.info(
                "Verifying all %d S3 manifest(s) have been loaded...", len(recent_s3_manifest_keys)
            )
            unprocessed_manifests = [
                s3_manifest
                for s3_manifest in recent_s3_manifest_keys
                if not any(
                    s3_manifest == db_manifest.s3_key and db_manifest.status == "COMPLETED"
                    for db_manifest in db_manifest_files
                )
            ]
            if len(unprocessed_manifests) == 0:
                logger.info("All manifests in S3 have been loaded by the Pipeline. Stopping")
                return

            # If we get here, there are unprocessed manifests. We need to send an alert to the alert
            # topic(s)
            logger.info(
                "%d manifest(s) failed to load over the weekend; publishing notification(s) to "
                "topic(s) %s; (manifest(s): %s)",
                len(unprocessed_manifests),
                ", ".join(arn.split(":")[-1] for arn in ALERT_TOPIC_ARNS),
                ", ".join(unprocessed_manifests),
            )
            sns_client = boto3.client("sns", config=BOTO_CONFIG)
            sns_message = create_splunk_sns_notification(
                unprocessed_manifests=unprocessed_manifests
            ).model_dump_json(by_alias=True)
            for topic_arn in ALERT_TOPIC_ARNS:
                logger.info("Publishing message to %s...", topic_arn)
                sns_client.publish(
                    TopicArn=topic_arn,
                    Message=sns_message,
                )
                logger.info("Published message to %s successfully", topic_arn)
    except Exception:
        logger.exception("Unrecoverable exception raised")
        raise
