#!/bin/bash
metadata_token_ttl_seconds=300

startup_log={{ db_migrator_dir }}/startup.log
touch $startup_log
log() {
  echo "$(date -u '+%Y-%m-%d %H:%M:%S') $1" >> "$startup_log"
}

export CCW_RIF_JOB_ENABLED='{{ data_pipeline_ccw_rif_job_enabled }}'
export S3_BUCKET_NAME='{{ data_pipeline_s3_bucket }}'
export HICN_HASH_ITERATIONS='{{ data_pipeline_hicn_hash_iterations }}'  # The minimum number of iterations recommended by NIST is 1000.
export HICN_HASH_PEPPER='{{ data_pipeline_hicn_hash_pepper }}'

DATABASE_URL="${DATABASE_URL:-"{{ data_pipeline_db_url }}"}"
DATABASE_USERNAME="${DATABASE_USERNAME:-{{ data_pipeline_db_username }}}"
DATABASE_PASSWORD="${DATABASE_PASSWORD:-{{ data_pipeline_db_password }}}"

# If DATABASE_USERNAME and DATABASE_PASSWORD is set to "iam", we are using iam for authentication. The username and password will be
# replaced with the actual values from the metadata service and the generate-db-auth-token command.
if [[ "$DATABASE_USERNAME" == "iam" && "$DATABASE_PASSWORD" == "iam" ]]; then
  log "IAM authentication enabled for database connection"

  # Get the hostname from the DATABASE_URL.
  DATABASE_HOST=$(echo "$DATABASE_URL" | sed -n 's/.*\/\/\(.*\):.*/\1/p')

  # Get the instance profile role from the metadata service. This is the database username.
  log "Retrieving auth token for $DATABASE_HOST"
  IMDSv2_TOKEN=$(curl -s -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: $metadata_token_ttl_seconds")
  DATABASE_USERNAME=$(curl -s -S -H "X-aws-ec2-metadata-token: $IMDSv2_TOKEN" http://169.254.169.254/latest/meta-data/iam/security-credentials/)
  for i in {1..3}; do
    if DATABASE_PASSWORD="$(aws rds generate-db-auth-token --hostname "$DATABASE_HOST" --port 5432 --username="$DATABASE_USERNAME")"; then
      log "OK"
      break
    else
      log "Failed to token for $DATABASE_HOST retrying... (attempt $i/3)"
      sleep 1
    fi
  done
  if [[ -z "$DATABASE_PASSWORD" ]]; then
    log "Failed to obtain token for $DATABASE_HOST after 3 attempts, exiting..."
    exit 1
  fi
fi
export DATABASE_URL
export DATABASE_USERNAME
export DATABASE_PASSWORD

# NOTE: RIF Loader Threads are generally some multiple of available cpus/core, e.g. `$((3 * $(nproc)))`
export LOADER_THREADS='{{ ansible_processor_vcpus * (rif_thread_multiple | int) }}'
export RIF_JOB_BATCH_SIZE='{{ rif_job_batch_size }}'
export RIF_JOB_QUEUE_SIZE_MULTIPLE='{{ rif_job_queue_size_multiple }}'
export LOADER_THREADS_CLAIMS='{{ ansible_processor_vcpus * (rif_thread_multiple_claims | int) }}'
export RIF_JOB_BATCH_SIZE_CLAIMS='{{ rif_job_batch_size_claims }}'
export RIF_JOB_QUEUE_SIZE_MULTIPLE_CLAIMS='{{ rif_job_queue_size_multiple_claims }}'

export IDEMPOTENCY_REQUIRED='{{ data_pipeline_idempotency_required }}'
export FILTERING_NON_NULL_AND_NON_2023_BENES='{{ data_pipeline_filtering_non_null_and_non_2023_benes }}'

# Observability
{% if data_pipeline_micrometer_cw_enabled is defined and data_pipeline_micrometer_cw_namespace is defined %}
export MICROMETER_CW_ENABLED={{ data_pipeline_micrometer_cw_enabled }}
export MICROMETER_CW_NAMESPACE={{ data_pipeline_micrometer_cw_namespace }}
export MICROMETER_CW_INTERVAL={{ data_pipeline_micrometer_cw_interval | default('PT1M') }}
{% else %}
# export MICROMETER_CW_ENABLED=
# export MICROMETER_CW_NAMESPACE=
# export MICROMETER_CW_INTERVAL=
{% endif %}

export NEW_RELIC_METRIC_HOST='{{ data_pipeline_new_relic_metric_host }}'
export NEW_RELIC_METRIC_PATH='{{ data_pipeline_new_relic_metric_path }}'
{% if data_pipeline_new_relic_app_name is defined %}
export NEW_RELIC_APP_NAME='{{ data_pipeline_new_relic_app_name }}'
{% else %}
#export NEW_RELIC_APP_NAME=
{% endif %}
{% if data_pipeline_new_relic_metric_key is defined %}
export NEW_RELIC_METRIC_KEY='{{ data_pipeline_new_relic_metric_key }}'
{% else %}
#export NEW_RELIC_METRIC_KEY=
{% endif %}

export RDA_JOB_ENABLED='{{ data_pipeline_rda_job_enabled }}'
export RDA_JOB_INTERVAL_SECONDS='{{ data_pipeline_rda_job_interval_seconds }}'
export RDA_JOB_BATCH_SIZE='{{ data_pipeline_rda_job_batch_size }}'
export RDA_JOB_WRITE_THREADS='{{ data_pipeline_rda_job_write_threads }}'
{% if data_pipeline_rda_process_dlq is defined %}
export RDA_JOB_PROCESS_DLQ='{{ data_pipeline_rda_process_dlq }}'
{% endif %}
{% if data_pipeline_rda_version is defined %}
export RDA_JOB_RDA_VERSION='{{ data_pipeline_rda_version }}'
{% endif %}
{% if data_pipeline_rda_job_starting_fiss_seq_num is defined %}
export RDA_JOB_STARTING_FISS_SEQ_NUM='{{ data_pipeline_rda_job_starting_fiss_seq_num }}'
{% endif %}
{% if data_pipeline_rda_job_starting_mcs_seq_num is defined %}
export RDA_JOB_STARTING_MCS_SEQ_NUM='{{ data_pipeline_rda_job_starting_mcs_seq_num }}'
{% endif %}
export RDA_GRPC_HOST='{{ data_pipeline_rda_grpc_host }}'
export RDA_GRPC_PORT='{{ data_pipeline_rda_grpc_port }}'
export RDA_GRPC_MAX_IDLE_SECONDS='{{ data_pipeline_rda_grpc_max_idle_seconds }}'
export RDA_GRPC_AUTH_TOKEN='{{ data_pipeline_rda_grpc_auth_token }}'
export RDA_GRPC_SERVER_TYPE='{{ data_pipeline_rda_grpc_server_type }}'
export RDA_GRPC_INPROC_SERVER_MODE='{{ data_pipeline_rda_grpc_inproc_server_mode }}'
export RDA_GRPC_INPROC_SERVER_S3_REGION='{{ data_pipeline_rda_grpc_inproc_server_s3_region }}'
export RDA_GRPC_INPROC_SERVER_S3_BUCKET='{{ data_pipeline_rda_grpc_inproc_server_s3_bucket }}'
export RDA_GRPC_INPROC_SERVER_S3_DIRECTORY='{{ data_pipeline_rda_grpc_inproc_server_s3_directory }}'

# Referenced by the pipeline's logback.xml file to add environment name for use in splunk.
export BFD_ENV_NAME='{{ env_name_std }}'


# Either don't set this variable, or set it to one of: BENEFICIARY, CARRIER, DME, HHA, HOSPICE, INPATIENT, OUTPATIENT, PDE, SNF
# export DATA_SET_TYPE_ALLOWED="BENEFICIARY"

# Set some additional variables.
JVM_ARGS='{{ data_pipeline_jvm_args }}'

exec "{{ data_pipeline_dir }}/bfd-pipeline-app-1.0.0-SNAPSHOT/bfd-pipeline-app.sh" \
	${JVM_ARGS} \
	-Djava.io.tmpdir={{ data_pipeline_tmp_dir }} -Dorg.jboss.logging.provider=slf4j \
	&>> "{{ data_pipeline_dir }}/bluebutton-data-pipeline.log"
