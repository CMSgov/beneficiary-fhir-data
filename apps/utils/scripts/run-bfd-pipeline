#!/bin/bash
# Runs the latest version of BFD pipeline using build artifacts from your git workspace.
# Requires a directory to copy build artifacts into and use as working directory.
# Can run the pipeline in remote debugging mode to enable setting breakpoints and debugging in IDEA.
# Run with -h option to see command line options.

set -e

if [[ x$BFD_PATH = x ]] ; then
  echo "Please set BFD_PATH environment variable to root of source tree." 1>&2
  echo "It should be the directory containing the ops and apps subdirectories." 1>&1
  exit 1
fi

if [[ x$BFD_EXEC = x ]] ; then
  echo "Please set BFD_EXEC environment variable to a directory scripts can run in." 1>&2
  echo "It should be outside of $BFD_PATH." 1>&2
  exit 1
fi

##############################################################################
#
# BEGIN Local environment configuration.
#
project_dir=$BFD_PATH/apps

# Directory to store binaries and data files when running.
# Also where the script cd's to before running.
# Should be outside of your git workspace to avoid hassles with git.
exec_dir=$BFD_EXEC/pipeline-app

# Directory containing your local maven artifacts cache.
repo_dir=$HOME/.m2/repository

# Maximum heap size to use when running
max_heap=4g

if [[ ! -d $project_dir ]] ; then
  echo ERROR: $project_dir does not exist. 1>&2
  echo Please set BFD_PATH to the root of the BFD source tree. 1>&2
  exit 1
fi

if [[ ! -d $BFD_EXEC ]] ; then
  echo ERROR: $BFD_EXEC does not exist. 1>&2
  echo Please create it or set BFD_EXEC to a different path. 1>&2
  exit 1
fi

if [[ ! -d $repo_dir ]] ; then
  echo ERROR: $repo_dir does not exist. 1>&2
  echo Please create it or modify this script with alternative path. 1>&2
  exit 1
fi

# ensure that the bfd-pipeline specific exec directory exists
mkdir -p $exec_dir

#
# END Local environment configuration.
#
##############################################################################

#
# Generic code using settings begins here.
#

# Default option values.
batch_size=20
cache_size=100
debug_enabled=false
debug_suspend=n
db_host=localhost
db_port=5432
idempotency_mode=false
interval_seconds=60
rda_host=localhost
rda_port=5003
rda_token=
s3_mode=minio
s3_bucket="bfd-pipeline"
s3_directory=test
threads=20
install=true
image_tag=
image_name=bfd-pipeline-app
localstack_ip=localhost
bene_threads=
bene_batch_size=
bene_queue_multiple=
claim_threads=
claim_batch_size=
claim_queue_multiple=

usage() {
  cat 1>&2 <<EOF
Runs a BFD pipeline.  Use command line options to change behaviors.

$(basename $0) [options] run_mode
-b batch_size             Number of entities per batch.
-c cache_size             Size of hicn/mbi cache.
-d db_host[:db_port]      Name of database host.  Optional :n uses port n.
-i image_tag              Runs the pipeline as a container using the image with given tag.
-I                        Enable RIF idempotency mode.
-l localstack_ip          Localstack IP address (for bridge networking).
-r run_interval           Job execution interval in seconds.
-R host:port:token        Host name, port, and token to call RDA API remote server.
-s mode:bucket:directory  S3 mode (minio or aws), bucket and directory.
-t threads                Number of threads for RIF/RDA jobs.
-x                        Do not install latest version before running.
-z                        Enable debugger support but start immediately.
-Z                        Enable debugger support and wait for debugger to connect.
-h                        Prints this help message.

Supported long options:

--bene-load-options threads:batch_size:queue_multiple
  Sets the RifLoader settings to use for beneficiary data.

--claim-load-options: threads:batch_size:queue_multiple
  Sets the RifLoader settings to use for claim data.

Run mode options: 
  rif     RIF pipeline only.
  random  RDA pipeline only.  Using in-process random API server.
  s3      RDA pipeline only.  Using in-process S3 API server.
  rda     RDA pipeline only.  Using remote RDA API server with host/port/token from command line.

Option defaults:
  -b $batch_size
  -c $cache_size
  -d $db_host:$db_port
  -l $localstack_ip
  -r $interval_seconds
  -R ${rda_host}:${rda_port}:${rda_token}
  -s ${s3_mode}:${s3_bucket}:${s3_directory}
  -t $threads
EOF
}

while getopts "b:c:d:i:Il:r:R:s:t:xzZh-:" option ; do
  case $option in
  b) batch_size=$OPTARG ;;
  c) cache_size=$OPTARG ;;
  d) IFS=':' read h p <<<"$OPTARG"
     db_host=${h:=$db_host}
     db_port=${p:=$db_port}
     unset h p
     ;;
  i) image_tag=$OPTARG ; install=false ;;
  I) idempotency_mode=true ;;
  l) localstack_ip=$OPTARG ;;
  r) interval_seconds=$OPTARG ;;
  R) IFS=':' read h p t <<<"$OPTARG"
     rda_host=${h:=$rda_host}
     rda_port=${p:=$rda_port}
     rda_token=${t:=$rda_token}
     unset h p t
     ;;
  s) IFS=':' read m b d <<<"$OPTARG"
     s3_mode=${m:=$s3_mode}
     s3_bucket=${b:=$s3_bucket}
     s3_directory=${d:=$s3_directory}
     unset m b d
     ;;
  t) threads=$OPTARG ;;
  x) install=false ;;
  z) debug_enabled=true ;;
  Z) debug_enabled=true ; debug_suspend=y ;;
  -) echo $OPTARG
     case $OPTARG in
       bene-load-options)
         val="${!OPTIND}"; OPTIND=$(( $OPTIND + 1 ))
         IFS=':' read t b q <<<"$val"
         bene_threads=$t
         bene_batch_size=$b
         bene_queue_multiple=$q
         unset t b q
         ;;
       claim-load-options)
         val="${!OPTIND}"; OPTIND=$(( $OPTIND + 1 ))
         IFS=':' read t b q <<<"$val"
         claim_threads=$t
         claim_batch_size=$b
         claim_queue_multiple=$q
         ;;
       *) usage ; exit 1
     esac
     ;;
  h) usage ; exit 0 ;;
  *) usage ; exit 1 ;;
  esac
done
shift $((OPTIND - 1))

# One other command line argument, the run_mode,is required.
if [[ $# != 1 ]] ; then
  usage
  exit 1
fi
run_mode=$1

if [[ x$image_tag != x ]] ; then
  echo "using docker image $image_name:$image_tag"
fi

# Autodetect if localstack is running based on whether we can connect to its port locally.
if lsof -Pi :4566 -sTCP:LISTEN -t >/dev/null ; then
  echo "using localstack"
  export SSM_REGION=us-east-1
  export SSM_ENDPOINT=http://${localstack_ip}:4566
  export SSM_PARAMETER_PATH=/bfd-pipeline
fi

# Either export an environment variable (if not using SSM for config) or set the appropriate
# parameter in SSM if we are using it.  Used to allow localstack based config to simulate using
# SSM in the cloud.
function setvar() {
  name="$1"
  value="$2"
  if [[ -z "$SSM_REGION" ]] ; then
    eval "export ${name}=\"${value}\""
  else
    echo put-parameter $name
    aws ssm put-parameter --overwrite --endpoint-url http://localhost:4566 --name "${SSM_PARAMETER_PATH}/${name}" --value "${value}" > /dev/null
  fi
}

ARGS="-Xms${max_heap} -Xmx${max_heap}"
ARGS="$ARGS -Dorg.jboss.logging.provider=slf4j"

# Uncomment for verbose GC logging.
# ARGS="$ARGS -XX:+PrintGCDetails"

# Uncomment to test GC string dedup mode.
# ARGS="$ARGS -XX:+UseG1GC -XX:+UseStringDeduplication" # -XX:+PrintStringDeduplicationStatistics"

# If debug_enabled is true an IDE can connect its debugger on port 5005 to debug the pipeline.
# If debug_suspend is y the pipeline app will wait for a debugger to connect before doing any work.
if [[ $debug_enabled = "true" ]] ; then
  ARGS="$ARGS -agentlib:jdwp=transport=dt_socket,server=y,suspend=${debug_suspend},address=*:5005"
fi

case $s3_mode in
minio) ARGS="$ARGS -Ds3.local=true" ;;
aws) ;;
*) echo error: s3 mode must be '"minio" or "aws"' 1>&2 ; usage ; exit 1 ;;
esac

if [[ $s3_directory != "" ]] && [[ $s3_bucket = "" ]] ; then
  echo "error: using s3 directory requires an s3 bucket" 1>&2
  usage
  exit 1
fi

# The pipeline app project will contain our runtime artifacts.
target_name=bfd-pipeline-app
lib_dir=${exec_dir}/lib

cd $exec_dir

# Don't just assume the version will always be 1.0.0-SNAPSHOT, get it dynamically from mvn instead.
project_version=`cd $project_dir ; mvn help:evaluate -Dexpression=project.version | grep -e '^[^\[]'`

# Install has to be true at least once so that the exec_dir can be populated with runtime artifacts.
# Subsequent runs have the option of skipping the install step for slightly faster turn around.
# Here we just copy the application zip file and extract its contents into the exec_dir for use
# in running the application.
if [[ $install = "true" ]] ; then
  binaries_dir=$repo_dir/gov/cms/bfd/${target_name}/${project_version}
  zip_file=${binaries_dir}/${target_name}-${project_version}.zip

  if [[ ! -r $zip_file ]] ; then
    echo ERROR: missing zip file: $zip_file 1>&2
    echo Build $target_name and try again. 1>&2
    exit 1
  fi

  app_name=`basename $zip_file .zip`
  unzip_dir=$app_name

  unzip -o $zip_file
  if [[ ! -d $unzip_dir ]] ; then
    echo ERROR: $zip_file did not contain expected root dir: $unzip_dir 1>&2
    exit 1
  fi

  jar_file=${exec_dir}/${app_name}.jar
  [[ -r $jar_file ]] && rm -f $jar_file
  [[ -d $lib_dir ]] && rm -rf $lib_dir
  [[ -r $run_script ]] && rm -f $run_script
  mv $unzip_dir/* $exec_dir
  rmdir $unzip_dir
  if [[ ! -r $jar_file ]] || [[ ! -d $lib_dir ]] ; then
    echo "ERROR: Missing jar file or lib directory.  Was zip file valid? (${zip_file})" 1>&2
    exit 1
  fi
else
  # Just check to be sure we can run without installing and fail if we can't.
  jar_file=`echo ${exec_dir}/${target_name}-${project_version}.jar`
  if [[ ! -r $jar_file ]] || [[ ! -d $lib_dir ]] ; then
    echo "ERROR: Missing $jar_file or lib directory." 1>&2
    echo "Run again without -x option to install binaries." 1>&2
    exit 1
  fi
fi

# Set up the configuration variables using the setvar function.
export BFD_ENV_NAME="laptop"
setvar S3_BUCKET_NAME "$s3_bucket"
setvar HICN_HASH_ITERATIONS "1000"  # The minimum number of iterations recommended by NIST is 1000.
setvar HICN_HASH_PEPPER "6f6e7474656865726c61657070707265" # nottherealpepper
setvar DATABASE_URL "jdbc:postgresql://${db_host}:${db_port}/bfd?logServerErrorDetail=false"
setvar DATABASE_USERNAME "bfd"
setvar DATABASE_PASSWORD "InsecureLocalDev"
setvar FILTERING_NON_NULL_AND_NON_2023_BENES "false"

#setvar NEW_RELIC_APP_NAME "-- data_pipeline_new_relic_app_name --"
#setvar NEW_RELIC_METRIC_HOST "-- data_pipeline_new_relic_metric_host --"
#setvar NEW_RELIC_METRIC_PATH "-- data_pipeline_new_relic_metric_path --"
#setvar NEW_RELIC_METRIC_KEY "-- data_pipeline_new_relic_metric_key --"

# For testing RIF performance.
if [[ x$bene_threads = x ]] ; then
  setvar LOADER_THREADS "$threads"
  setvar RIF_JOB_BATCH_SIZE "$batch_size"
  setvar RIF_JOB_QUEUE_SIZE_MULTIPLE "2"
else
  setvar LOADER_THREADS "$bene_threads"
  setvar RIF_JOB_BATCH_SIZE "$bene_batch_size"
  setvar RIF_JOB_QUEUE_SIZE_MULTIPLE "$bene_queue_multiple"
fi

if [[ x$claim_threads != x ]] ; then
  setvar LOADER_THREADS_CLAIMS "$claim_threads"
  setvar RIF_JOB_BATCH_SIZE_CLAIMS "$claim_batch_size"
  setvar RIF_JOB_QUEUE_SIZE_MULTIPLE_CLAIMS "$claim_queue_multiple"
fi

setvar HICN_HASH_CACHE_SIZE "$cache_size"
setvar IDEMPOTENCY_REQUIRED "$idempotency_mode"

setvar RDA_JOB_INTERVAL_SECONDS "$interval_seconds"
setvar RDA_JOB_BATCH_SIZE "$batch_size"
setvar RDA_JOB_WRITE_THREADS "$threads"
setvar RDA_JOB_PROCESS_DLQ "true"
setvar RDA_JOB_ERROR_EXPIRE_DAYS "5"

# For testing RDA ingestion in a loop.
#setvar RDA_JOB_STARTING_FISS_SEQ_NUM "0"
#setvar RDA_JOB_STARTING_MCS_SEQ_NUM "0"

setvar MICROMETER_JMX_ENABLED "true"

# Command line option below determines which jobs to run.
setvar CCW_RIF_JOB_ENABLED "false"
setvar RDA_JOB_ENABLED "false"

case $run_mode in
  rif)
    setvar CCW_RIF_JOB_ENABLED "true"
    ;;
  random)
    setvar RDA_JOB_ENABLED "true"
    setvar RDA_GRPC_SERVER_TYPE "InProcess"
    setvar RDA_GRPC_INPROC_SERVER_MODE "Random"
    setvar RDA_GRPC_INPROC_SERVER_RANDOM_SEED "42"
    setvar RDA_GRPC_INPROC_SERVER_RANDOM_MAX_CLAIMS "25000"
    ;;
  rda)
    setvar RDA_JOB_ENABLED "true"
    setvar RDA_GRPC_SERVER_TYPE "Remote"
    setvar RDA_GRPC_HOST "$rda_host"
    setvar RDA_GRPC_PORT "$rda_port"
    setvar RDA_GRPC_MAX_IDLE_SECONDS "600"
    setvar RDA_GRPC_AUTH_TOKEN "$rda_token"
    ;;
  s3)
    setvar RDA_JOB_ENABLED "true"
    setvar RDA_GRPC_SERVER_TYPE "InProcess"
    setvar RDA_GRPC_INPROC_SERVER_MODE "S3"
    setvar RDA_GRPC_INPROC_SERVER_S3_REGION ""
    setvar RDA_GRPC_INPROC_SERVER_S3_BUCKET "$s3_bucket"
    setvar RDA_GRPC_INPROC_SERVER_S3_DIRECTORY "$s3_directory"
    ;;
  *)
    echo Invalid value for run_mode: $run_mode 1>&2
    exit 1
esac

classpath="${jar_file}:${exec_dir}/lib/*"
mainClass="gov.cms.bfd.pipeline.app.PipelineApplication"
[ -n "${JAVA_HOME}" ] && java=${JAVA_HOME}/bin/java || java=java

if [[ x$image_tag = x ]] ; then
  echo running application directly
  export CLASSPATH="$classpath"
  exec $java $ARGS $mainClass
else
  echo running container
  docker \
    run \
    --rm \
    --name $image_name \
    --env SSM_REGION \
    --env SSM_ENDPOINT \
    --env SSM_PARAMETER_PATH \
    --env HOME=/home/app \
    --env BFD_ENV_NAME="$BFD_ENV_NAME" \
    -v $HOME/.aws/credentials:/home/app/.aws/credentials:ro \
    $image_name:$image_tag
fi
