#!/bin/bash
# Runs the latest version of BFD pipeline using build artifacts from your git workspace.
# Requires a directory to copy build artifacts into and use as working directory.
# Can run the pipeline in remote debugging mode to enable setting breakpoints and debugging in IDEA.
# Run with -h option to see command line options.

set -e

if [[ x$BFD_PATH = x ]] ; then
  echo "Please set BFD_PATH environment variable to root of source tree." 1>&2
  echo "It should be the directory containing the ops and apps subdirectories." 1>&1
  exit 1
fi

if [[ x$BFD_EXEC = x ]] ; then
  echo "Please set BFD_EXEC environment variable to a directory scripts can run in." 1>&2
  echo "It should be outside of $BFD_PATH." 1>&2
  exit 1
fi

##############################################################################
#
# BEGIN Local environment configuration.
#
project_dir=$BFD_PATH/apps

# Directory to store binaries and data files when running.
# Also where the script cd's to before running.
# Should be outside of your git workspace to avoid hassles with git.
exec_dir=$BFD_EXEC/pipeline-app

# Directory containing your local maven artifacts cache.
repo_dir=$HOME/.m2/repository

# Maximum heap size to use when running
max_heap=4g

# Location of kubernetes configurations.
kubernetes_root=$BFD_PATH/ops/k8s/helm

if [[ ! -d $project_dir ]] ; then
  echo ERROR: $project_dir does not exist. 1>&2
  echo Please set BFD_PATH to the root of the BFD source tree. 1>&2
  exit 1
fi

if [[ ! -d $BFD_EXEC ]] ; then
  echo ERROR: $BFD_EXEC does not exist. 1>&2
  echo Please create it or set BFD_EXEC to a different path. 1>&2
  exit 1
fi

if [[ ! -d $repo_dir ]] ; then
  echo ERROR: $repo_dir does not exist. 1>&2
  echo Please create it or modify this script with alternative path. 1>&2
  exit 1
fi

# ensure that the bfd-pipeline specific exec directory exists
mkdir -p $exec_dir

#
# END Local environment configuration.
#
##############################################################################

#
# Generic code using settings begins here.
#

# Default option values.
batch_size=20
cache_size=100
debug_enabled=false
debug_suspend=n
db_host=localhost
db_port=5432
idempotency_mode=false
interval_seconds=60
rda_host=localhost
rda_port=5003
rda_token=
s3_required=false
s3_mode=local
s3_bucket="bfd-pipeline"
s3_directory=test
threads=20
install=true
image_tag=
image_name=bfd-pipeline-app
localstack_ip=localhost
bene_threads=
bene_batch_size=
bene_queue_multiple=
claim_threads=
claim_batch_size=
claim_queue_multiple=
kubernetes=false

usage() {
  cat 1>&2 <<EOF
Runs a BFD pipeline.  Use command line options to change behaviors.

$(basename $0) [options] run_mode
-b batch_size             Number of entities per batch.
-c cache_size             Size of hicn/mbi cache.
-d db_host[:db_port]      Name of database host.  Optional :n uses port n.
-i image_tag              Runs the pipeline as a container using the image with given tag.
-I                        Enable RIF idempotency mode.
-k                        Run using kubernetes (using ops/k8s).
-l localstack_ip          Localstack IP address (for bridge networking).
-r run_interval           Job execution interval in seconds.
-R host:port:token        Host name, port, and token to call RDA API remote server.
-s mode:bucket:directory  S3 mode (local or aws), bucket and directory.
-t threads                Number of threads for RIF/RDA jobs.
-x                        Do not install latest version before running.
-z                        Enable debugger support but start immediately.
-Z                        Enable debugger support and wait for debugger to connect.
-h                        Prints this help message.

Supported long options:

--bene-load-options threads:batch_size:queue_multiple
  Sets the RifLoader settings to use for beneficiary data.

--claim-load-options threads:batch_size:queue_multiple
  Sets the RifLoader settings to use for claim data.

Run mode options: 
  rif     RIF pipeline only.
  random  RDA pipeline only.  Using in-process random API server.
  s3      RDA pipeline only.  Using in-process S3 API server.
  rda     RDA pipeline only.  Using remote RDA API server with host/port/token from command line.

Option defaults:
  -b $batch_size
  -c $cache_size
  -d $db_host:$db_port
  -l $localstack_ip
  -r $interval_seconds
  -R ${rda_host}:${rda_port}:${rda_token}
  -s ${s3_mode}:${s3_bucket}:${s3_directory}
  -t $threads
EOF
}

while getopts "b:c:d:i:Ikl:r:R:s:t:xzZh-:" option ; do
  case $option in
  b) batch_size=$OPTARG ;;
  c) cache_size=$OPTARG ;;
  d) IFS=':' read h p <<<"$OPTARG"
     db_host=${h:=$db_host}
     db_port=${p:=$db_port}
     unset h p
     ;;
  i) image_tag=$OPTARG ; install=false ;;
  I) idempotency_mode=true ;;
  k) kubernetes=true ; install=false ; db_host=dev-postgres ;;
  l) localstack_ip=$OPTARG ;;
  r) interval_seconds=$OPTARG ;;
  R) IFS=':' read h p t <<<"$OPTARG"
     rda_host=${h:=$rda_host}
     rda_port=${p:=$rda_port}
     rda_token=${t:=$rda_token}
     unset h p t
     ;;
  s) IFS=':' read m b d <<<"$OPTARG"
     s3_mode=${m:=$s3_mode}
     s3_bucket=${b:=$s3_bucket}
     s3_directory=${d:=$s3_directory}
     unset m b d
     ;;
  t) threads=$OPTARG ;;
  x) install=false ;;
  z) debug_enabled=true ;;
  Z) debug_enabled=true ; debug_suspend=y ;;
  -) case $OPTARG in
       bene-load-options)
         val="${!OPTIND}"; OPTIND=$(( $OPTIND + 1 ))
         IFS=':' read t b q <<<"$val"
         bene_threads=$t
         bene_batch_size=$b
         bene_queue_multiple=$q
         unset t b q
         ;;
       claim-load-options)
         val="${!OPTIND}"; OPTIND=$(( $OPTIND + 1 ))
         IFS=':' read t b q <<<"$val"
         claim_threads=$t
         claim_batch_size=$b
         claim_queue_multiple=$q
         ;;
       *) usage ; exit 1
     esac
     ;;
  h) usage ; exit 0 ;;
  *) usage ; exit 1 ;;
  esac
done
shift $((OPTIND - 1))

# One other command line argument, the run_mode,is required.
if [[ $# != 1 ]] ; then
  usage
  exit 1
fi
run_mode=$1

if [[ $run_mode = rif ]] || [[ $run_mode = s3 ]] ; then
  s3_required=true
fi

if [[ x$image_tag != x ]] ; then
  echo "using docker image $image_name:$image_tag"
fi

# Autodetect if localstack is running based on whether we can connect to its port locally.
localstack_running=false
if lsof -Pi :4566 -sTCP:LISTEN -t >/dev/null ; then
  echo "using localstack"
  localstack_running=true
  export AWS_REGION=us-east-1
  export AWS_ENDPOINT=http://${localstack_ip}:4566/
  SSM_PARAMETER_PATH=/bfd-pipeline
  export CONFIG_SETTINGS_JSON=$(cat <<EOF
{
  "ssmHierarchies": [ "$SSM_PARAMETER_PATH" ]
}
EOF
)
fi

# Either export an environment variable (if not using SSM for config) or set the appropriate
# parameter in SSM if we are using it.  Used to allow localstack based config to simulate using
# SSM in the cloud.
function setvar() {
  env_name="$1"
  ssm_name="$2"
  value="$3"
  if [[ $localstack_running = "false" ]] ; then
    eval "export ${env_name}=\"${value}\""
  else
    echo put-parameter $name
    aws ssm put-parameter --overwrite --endpoint-url http://localhost:4566 --name "${SSM_PARAMETER_PATH}/${ssm_name}" --value "${value}" > /dev/null
  fi
}

ARGS="-Xms${max_heap} -Xmx${max_heap}"
ARGS="$ARGS -Dorg.jboss.logging.provider=slf4j"

# Uncomment for verbose GC logging.
# ARGS="$ARGS -XX:+PrintGCDetails"

# Uncomment to test GC string dedup mode.
# ARGS="$ARGS -XX:+UseG1GC -XX:+UseStringDeduplication" # -XX:+PrintStringDeduplicationStatistics"

# If debug_enabled is true an IDE can connect its debugger on port 5005 to debug the pipeline.
# If debug_suspend is y the pipeline app will wait for a debugger to connect before doing any work.
if [[ $debug_enabled = "true" ]] ; then
  ARGS="$ARGS -agentlib:jdwp=transport=dt_socket,server=y,suspend=${debug_suspend},address=*:5005"
fi

if [[ $s3_required = "true" ]] ; then
  case $s3_mode in
  local)
    if [[ $localstack_running = "false" ]] ; then
      echo error: s3 local mode requires localstack to be running 1>&2
      usage
      exit 1
    fi
    ;;
  aws) ;;
  *) echo error: s3 mode must be '"local" or "aws"' 1>&2 ; usage ; exit 1 ;;
  esac

  if [[ $s3_directory != "" ]] && [[ $s3_bucket = "" ]] ; then
    echo "error: using s3 directory requires an s3 bucket" 1>&2
    usage
    exit 1
  fi
fi

# The pipeline app project will contain our runtime artifacts.
target_name=bfd-pipeline-app
lib_dir=${exec_dir}/lib

cd $exec_dir

# Don't just assume the version will always be 1.0.0-SNAPSHOT, get it dynamically from mvn instead.
project_version=`cd $project_dir ; mvn help:evaluate -Dexpression=project.version | grep -e '^[^\[]'`

# Install has to be true at least once so that the exec_dir can be populated with runtime artifacts.
# Subsequent runs have the option of skipping the install step for slightly faster turn around.
# Here we just copy the application zip file and extract its contents into the exec_dir for use
# in running the application.
if [[ $install = "true" ]] ; then
  binaries_dir=$repo_dir/gov/cms/bfd/${target_name}/${project_version}
  zip_file=${binaries_dir}/${target_name}-${project_version}.zip

  if [[ ! -r $zip_file ]] ; then
    echo ERROR: missing zip file: $zip_file 1>&2
    echo Build $target_name and try again. 1>&2
    exit 1
  fi

  app_name=`basename $zip_file .zip`
  unzip_dir=$app_name

  unzip -o $zip_file
  if [[ ! -d $unzip_dir ]] ; then
    echo ERROR: $zip_file did not contain expected root dir: $unzip_dir 1>&2
    exit 1
  fi

  jar_file=${exec_dir}/${app_name}.jar
  [[ -r $jar_file ]] && rm -f $jar_file
  [[ -d $lib_dir ]] && rm -rf $lib_dir
  [[ -r $run_script ]] && rm -f $run_script
  mv $unzip_dir/* $exec_dir
  rmdir $unzip_dir
  if [[ ! -r $jar_file ]] || [[ ! -d $lib_dir ]] ; then
    echo "ERROR: Missing jar file or lib directory.  Was zip file valid? (${zip_file})" 1>&2
    exit 1
  fi
elif [[ x$image_tag = x ]] && [[ x$kubernetes = xfalse ]] ; then
  # Just check to be sure we can run without installing and fail if we can't.
  jar_file=`echo ${exec_dir}/${target_name}-${project_version}.jar`
  if [[ ! -r $jar_file ]] || [[ ! -d $lib_dir ]] ; then
    echo "ERROR: Missing $jar_file or lib directory." 1>&2
    echo "Run again without -x option to install binaries." 1>&2
    exit 1
  fi
fi

# Set up the configuration variables using the setvar function.
export BFD_ENV_NAME="laptop"
setvar BFD_CCW_S3_BUCKET_NAME ccw/s3_bucket_name S3_BUCKET_NAME "$s3_bucket"
setvar BFD_HICN_HASH_ITERATIONS hicn_hash/iterations "1000"  # The minimum number of iterations recommended by NIST is 1000.
setvar BFD_HICN_HASH_PEPPER hicn_hash/pepper "6f6e7474656865726c61657070707265" # nottherealpepper
setvar BFD_DB_URL db/url "jdbc:postgresql://${db_host}:${db_port}/bfd?logServerErrorDetail=false"
setvar BFD_DB_USERNAME db/username "bfd"
setvar BFD_DB_PASSWORD db/password "InsecureLocalDev"

#setvar NEW_RELIC_APP_NAME "-- data_pipeline_new_relic_app_name --"
#setvar NEW_RELIC_METRIC_HOST "-- data_pipeline_new_relic_metric_host --"
#setvar NEW_RELIC_METRIC_PATH "-- data_pipeline_new_relic_metric_path --"
#setvar NEW_RELIC_METRIC_KEY "-- data_pipeline_new_relic_metric_key --"

# For testing RIF performance.
if [[ x$bene_threads = x ]] ; then
  setvar BFD_LOADER_THREAD_COUNT loader_thread_count "$threads"
  setvar BFD_CCW_JOB_BATCH_SIZE ccw/job/batch_size "$batch_size"
  setvar BFD_CCW_JOB_QUEUE_SIZE_MULTIPLE ccw/job/queue_size_multiple "2"
else
  setvar BFD_LOADER_THREAD_COUNT loader_thread_count "$bene_threads"
  setvar BFD_CCW_JOB_BATCH_SIZE ccw/job/batch_size "$bene_batch_size"
  setvar BFD_CCW_JOB_QUEUE_SIZE_MULTIPLE ccw/job/queue_size_multiple "$bene_queue_multiple"
fi

if [[ x$claim_threads != x ]] ; then
  setvar BFD_CCW_JOB_CLAIMS_LOADER_THREAD_COUNT ccw/job/claims/loader_thread_count "$claim_threads"
  setvar BFD_CCW_JOB_CLAIMS_BATCH_SIZE ccw/job/claims/batch_size "$claim_batch_size"
  setvar BFD_CCW_JOB_CLAIMS_QUEUE_SIZE_MULTIPLE ccw/job/claims/queue_size_multiple "$claim_queue_multiple"
fi

setvar BFD_HICN_HASH_CACHE_SIZE hicn_hash/cache_size "$cache_size"
setvar BFD_CCW_IDEMPOTENCY_ENABLED ccw/idempotency_enabled "$idempotency_mode"

setvar BFD_RDA_JOB_INTERVAL_SECONDS rda/job/interval_seconds "$interval_seconds"
setvar BFD_RDA_JOB_BATCH_SIZE rda/job/batch_size "$batch_size"
setvar BFD_RDA_JOB_WRITE_THREAD_COUNT rda/job/write_thread_count "$threads"
setvar BFD_RDA_JOB_PROCESS_DLQ rda/job/process_dlq "true"
setvar BFD_RDA_JOB_ERROR_EXPIRE_DAYS rda/job/error_expire_days "5"

# For testing RDA ingestion in a loop.
#setvar BFD_RDA_JOB_STARTING_FISS_SEQ_NUM rda/job/starting_fiss_seq_num "0"
#setvar BFD_RDA_JOB_STARTING_MCS_SEQ_NUM rda/job/starting_mcs_seq_num "0"

setvar BFD_MICROMETER_CW_JMX_ENABLED micrometer_cw/jmx_enabled "true"

# Command line option below determines which jobs to run.
setvar BFD_CCW_JOB_ENABLED ccw/job/enabled "false"
setvar BFD_RDA_JOB_ENABLED rda/job/enabled "false"

case $run_mode in
  rif)
    setvar BFD_CCW_JOB_ENABLED ccw/job/enabled "true"
    ;;
  random)
    setvar BFD_RDA_JOB_ENABLED rda/job/enabled "true"
    setvar BFD_RDA_GRPC_SERVER_TYPE rda/grpc/server_type "InProcess"
    setvar BFD_RDA_GRPC_INPROCESS_SERVER_MODE rda/grpc/inprocess_server/mode "Random"
    setvar BFD_RDA_GRPC_INPROCESS_SERVER_RANDOM_SEED rda/grpc/inprocess_server/random/seed "42"
    setvar BFD_RDA_GRPC_INPROCESS_SERVER_RANDOM_MAX_CLAIMS rda/grpc/inprocess_server/random/max_claims "2500"
    ;;
  rda)
    setvar BFD_RDA_JOB_ENABLED rda/job/enabled "true"
    setvar BFD_RDA_GRPC_SERVER_TYPE rda/grpc/server_type "Remote"
    setvar BFD_RDA_GRPC_HOST rda/grpc/host "$rda_host"
    setvar BFD_RDA_GRPC_PORT rda/grpc/port "$rda_port"
    setvar BFD_RDA_GRPC_MAX_IDLE_SECONDS rda/grpc/max_idle_seconds "600"
    setvar BFD_RDA_GRPC_AUTH_TOKEN rda/grpc/auth_token "$rda_token"
    ;;
  s3)
    setvar BFD_RDA_JOB_ENABLED rda/job/enabled "true"
    setvar BFD_RDA_GRPC_SERVER_TYPE rda/grpc/server_type "InProcess"
    setvar BFD_RDA_GRPC_INPROCESS_SERVER_MODE rda/grpc/inprocess_server/mode "S3"
    setvar BFD_RDA_GRPC_INPROCESS_SERVER_S3_BUCKET rda/grpc/inprocess_server/s3/bucket "$s3_bucket"
    setvar BFD_RDA_GRPC_INPROCESS_SERVER_S3_DIR rda/grpc/inprocess_server/s3/dir "$s3_directory"
    ;;
  *)
    echo Invalid value for run_mode: $run_mode 1>&2
    exit 1
esac

classpath="${jar_file}:${exec_dir}/lib/*"
mainClass="gov.cms.bfd.pipeline.app.PipelineApplication"
[ -n "${JAVA_HOME}" ] && java=${JAVA_HOME}/bin/java || java=java

if [[ x$kubernetes = xtrue ]] ; then
  cd $kubernetes_root
  if [[ ! -r pipeline/Chart.yaml ]] ; then
    echo ERROR: unable to find kubernetes resource files in $kubernetes_root 1>&2
    exit 1
  fi
  if [[ $run_mode = "rif" ]] ; then
    pipeline_type="rif"
  else
    pipeline_type="rda"
  fi
  echo running kubernetes job
  echo Ignore \"release: not found\" message if this is first time running in cluster.
  helm -n dev uninstall pipeline-$pipeline_type || true
  helm -n dev install pipeline-$pipeline_type pipeline --set pipelineType=$pipeline_type
elif [[ x$image_tag = x ]] ; then
  echo running application directly
  export CLASSPATH="$classpath"
  exec $java $ARGS $mainClass
else
  echo running container
  docker \
    run \
    --rm \
    --name $image_name \
    --env AWS_REGION \
    --env AWS_ENDPOINT \
    --env CONFIG_SETTINGS_JSON \
    --env HOME=/home/app \
    --env BFD_ENV_NAME="$BFD_ENV_NAME" \
    -v $HOME/.aws/credentials:/home/app/.aws/credentials:ro \
    $image_name:$image_tag
fi
