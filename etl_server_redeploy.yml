---

##
# Gracefully stops and terminates the old ETL server (if it exists). Then, 
# ensures that the S3 bucket that ETL will pull from exists. Finally, 
# provisions a new ETL server instance.
##

- name: AWS Preparation for ETL Service
  hosts: localhost
  connection: local
  gather_facts: false

  tasks:

    - name: Create S3 Bucket for ETL
      s3_bucket:
        name: "{{ s3_bucket_etl_staging }}"

    - name: Search EC2 Instances
      ec2_remote_facts:
        region: "{{ aws_region }}"
        filters:
          'instance-state-name': 'running'
          'tag:Name': 'bluebutton-backend-etl'
      register: ec2_search_etl_old

    - name: Add ETL Server (if found) to Inventory (transient)
      add_host:
        name: backend_etl_old
        ansible_user: "{{ ssh_user_rhel }}"
        ansible_host: "{{ item.public_dns_name }}"
        ec2_instance_id: "{{ item.id }}"
      with_items: "{{ ec2_search_etl_old.instances }}"

- name: Shutdown Old ETL (if found)
  hosts: backend_etl_old
  tasks:

    # If a data set is currently being processed, this could take hours to
    # complete. That's perhaps inconvenient, but nonetheless fine. Stopping
    # processing in the middle of a data set would be bad.
    #- name: Stop ETL Service Permanently
    #  systemd:
    #    name: bluebutton-data-pipeline-app
    #    state: stopped
    #    enabled: false
    #    #no_block: false
    #  become: true

    - name: Terminate ETL Instance in EC2
      ec2:
        state: absent
        instance_ids: "{{ ec2_instance_id }}"
        region: "{{ aws_region }}"
      delegate_to: localhost

- name: Provision ETL Server
  hosts: localhost
  connection: local
  gather_facts: false

  tasks:

    - name: Provision ETL Server
      ec2:
        key_name: "{{ ec2_key_name }}"
        group:
          - default
          - ssh-all
        instance_type: "{{ ec2_backend_etl_instance_type }}"
        image: "{{ ami_id_rhel_7_encrypted }}"
        region: "{{ aws_region }}"
        zone: "{{ aws_zone }}"
        vpc_subnet_id: "{{ aws_vpc_subnet }}"
        volumes:
          - device_name: /dev/sda1
            volume_type: gp2
            # The sample RIF data set is about 92GB total.
            volume_size: 200
        instance_profile_name: BlueButtonBackend-EC2Services
        wait: true
        exact_count: 1
        count_tag:
          Name: bluebutton-backend-etl
        instance_tags:
          Name: bluebutton-backend-etl
          Application: "{{ ec2_tag_application }}"
          CreatedBy: "{{ whoami.stdout }}"
      register: ec2_backend_etl_new
    
    - name: Add ETL Server to Inventory (transient)
      add_host:
        name: backend_etl_new
        ansible_user: "{{ ssh_user_rhel }}"
        ansible_host: "{{ ec2_backend_etl_new.instances[0].public_dns_name }}"
    
    - name: Wait for SSH
      wait_for:
        host: "{{ item.public_dns_name }}"
        port: 22
        search_regex: OpenSSH
        state: started
        # This delay seems to be necessary for newly-provisioned instances.
        # SSH will be up but it's not immediately configured to accept the SSH 
        # key.
        delay: 30
        timeout: 320
      with_flattened:
        - "{{ ec2_backend_etl_new.instances }}"

- name: Configure ETL for Ansible Pipelining
  hosts: backend_etl_new
  vars:
    ansible_ssh_pipelining: false
  roles:
    - rhel_ansible_pipelining

- name: Ensure FHIR Database Exists (Using ETL Server)
  hosts: backend_etl_new
  tasks:

    - name: Install Pre-requisites
      yum:
        name: "{{ item }}"
        state: latest
      with_items:
        - python-psycopg2
      become: true
    
    - name: Create FHIR Database
      postgresql_db:
        login_host: "{{ hostvars['localhost']['backend_postgres_endpoint'] }}"
        port: "{{ hostvars['localhost']['backend_postgres_port'] }}"
        login_user: "{{ backend_postgres_master_username }}"
        login_password: "{{ backend_postgres_master_password }}"
        name: bluebutton_backend_db
        encoding: 'UNICODE'
        lc_collate: 'C'
        lc_ctype: 'C'
        template: 'template0'

- name: Configure ETL
  hosts: backend_etl_new
  roles:
    - role: karlmdavis.bluebutton_data_pipeline
      data_pipeline_appjar_name: "bluebutton-data-pipeline-app-{{ backend_etl_version }}-capsule-fat.jar"
      data_pipeline_appjar_s3_bucket: "{{ s3_bucket_deployment }}"
      data_pipeline_s3_bucket: "{{ s3_bucket_etl_staging }}"
      data_pipeline_hicn_hash_iterations: 2  # NIST recommends at least 1000, but this isn't production.
      data_pipeline_hicn_hash_pepper: '6E6F747468657265616C706570706572'  # Hex-encoded "nottherealpepper".
      data_pipeline_db_url: "jdbc:postgresql://{{ hostvars['localhost']['backend_postgres_endpoint'] }}:{{ hostvars['localhost']['backend_postgres_port'] }}/bluebutton_backend_db"
      data_pipeline_db_username: "{{ backend_postgres_master_username }}"
      data_pipeline_db_password: "{{ backend_postgres_master_password }}"

