---
##
# This playbook stands up the systems in AWS needed for one benchmark 
# iteration, uses them to run the benchmark, and then destroys the systems.
#
# Usage:
# 
#     $ ansible-playbook benchmark_etl.yml --extra-vars "iteration_index=42"
##

- name: Remove 'requiretty' Setting
  hosts: fhir:etl
  user: "{{ ssh_user }}"
  gather_facts: true
  vars:
    ansible_ssh_pipelining: false
  
  tasks:
    
    - lineinfile: regexp="^Defaults    requiretty" dest=/etc/sudoers state=absent
      become: true

- name: Create FHIR DB
  hosts: etl
  user: "{{ ssh_user }}"
  gather_facts: true

  tasks:

    - name: Install Pre-requisites
      yum:
        name: "{{ item }}"
        state: latest
      with_items:
        # Needed to use the `postgresql_db` module.
        - python-psycopg2
      become: true
    
    - name: Create FHIR DB
      postgresql_db:
        login_host: "{{ hostvars['rds']['endpoint'] }}"
        port: "{{ hostvars['rds']['port'] }}"
        login_user: "{{ postgres_master_username }}"
        login_password: "{{ postgres_master_password }}"
        name: fhirdb
        encoding: 'UNICODE'
        lc_collate: 'C'
        lc_ctype: 'C'
        template: 'template0'

#- name: Configure and Start FHIR Server
#  hosts: fhir
#  user: "{{ ssh_user }}"
#  gather_facts: true
#  roles:
#    - server_fhir

- name: Configure ETL Server
  hosts: etl
  user: "{{ ssh_user }}"
  gather_facts: true
  roles:
    - role: karlmdavis.bluebutton_data_pipeline
      data_pipeline_appjar_name: "bluebutton-data-pipeline-app-capsule-fat.jar"
      data_pipeline_appjar_s3_bucket: "{{ s3_benchmark_resources_bucket }}"
      data_pipeline_s3_bucket: "gov-hhs-cms-bluebutton-datapipeline-benchmark-iteration{{ iteration_index }}"
      data_pipeline_hicn_hash_iterations: 2  # NIST recommends at least 1000, but this isn't production.
      data_pipeline_hicn_hash_pepper: '6E6F747468657265616C706570706572'  # Hex-encoded "nottherealpepper".
      data_pipeline_db_url: "jdbc:postgresql://{{ hostvars['rds']['endpoint'] }}:{{ hostvars['rds']['port'] }}/fhirdb"
      data_pipeline_db_username: "{{ postgres_master_username }}"
      data_pipeline_db_password: "{{ postgres_master_password }}"
      data_pipeline_jvm_args: '-Xmx40g'
      data_pipeline_loader_threads: "{{ etl_loader_threads }}"

- name: Run Benchmark
  hosts: etl
  user: "{{ ssh_user }}"
  gather_facts: true
  
  tasks:
    
    - name: Wait for ETL to Start Processing Data Set
      wait_for:
        path: '/usr/local/bluebutton-data-pipeline/bluebutton-data-pipeline.log'
        search_regex: 'Data set ready. Processing it...'
        timeout: 60

    # This will take ages, so we watch it asynchronously, to ensure that SSH 
    # doesn't time out on us.
    - name: Wait for ETL to Finish Processing Data Set
      wait_for:
        path: '/usr/local/bluebutton-data-pipeline/bluebutton-data-pipeline.log'
        search_regex: '(Data set processing complete.|Application has finished shutting down.)'
        timeout: 10800
      # Let it run for 3h, checking it every 30s to see if it's done.
      async: 10800
      poll: 30

