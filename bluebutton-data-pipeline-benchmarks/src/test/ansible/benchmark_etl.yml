---
##
# This playbook stands up the systems in AWS needed for one benchmark 
# iteration, uses them to run the benchmark, and then destroys the systems.
#
# Usage:
# 
#     $ ansible-playbook benchmark_etl.yml --extra-vars "iteration_index=42"
##

- name: Remove 'requiretty' Setting
  hosts: fhir:etl
  user: "{{ ssh_user }}"
  gather_facts: true
  vars:
    ansible_ssh_pipelining: false
  
  tasks:
    
    - lineinfile: regexp="^Defaults    requiretty" dest=/etc/sudoers state=absent
      become: true

- name: Configure and Start FHIR Server
  hosts: fhir
  user: "{{ ssh_user }}"
  gather_facts: true
  roles:
    - server_fhir

- name: Configure ETL Server
  hosts: etl
  user: "{{ ssh_user }}"
  gather_facts: true
  roles:
    - server_etl

- name: Run Benchmark
  hosts: etl
  user: "{{ ssh_user }}"
  gather_facts: true
  
  tasks:
    
    - name: Wait for ETL to Start Processing Data Set
      wait_for:
        path: "/usr/local/bluebutton-data-pipeline-app/{{ iteration_index }}_bluebutton-data-pipeline-app.log"
        search_regex: 'Data set finished uploading and ready to process'
        timeout: 60

    # This will take ages, so we watch it asynchronously, to ensure that SSH 
    # doesn't time out on us.
    - name: Wait for ETL to Finish Processing Data Set
      wait_for:
        path: "/usr/local/bluebutton-data-pipeline-app/{{ iteration_index }}_bluebutton-data-pipeline-app.log"
        search_regex: 'Data set deleted, now that processing is complete'
        timeout: 28800
      # Let it run for 8h, checking it every 30s to see if it's done.
      async: 28800
      poll: 30
